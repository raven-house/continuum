{"noir_version":"1.0.0-beta.7+24c053fba747770cf8d3f813d22cfa003714dfb6","hash":"1350562857137862537","abi":{"parameters":[{"name":"note","type":{"kind":"struct","path":"nft_note::NFTNote","fields":[{"name":"owner","type":{"kind":"struct","path":"aztec::protocol_types::address::aztec_address::AztecAddress","fields":[{"name":"inner","type":{"kind":"field"}}]}},{"name":"randomness","type":{"kind":"field"}},{"name":"token_id","type":{"kind":"field"}}]},"visibility":"private"},{"name":"membership_witness","type":{"kind":"struct","path":"aztec::protocol_types::merkle_tree::membership::MembershipWitness","fields":[{"name":"leaf_index","type":{"kind":"field"}},{"name":"sibling_path","type":{"kind":"array","length":40,"type":{"kind":"field"}}}]},"visibility":"private"},{"name":"note_nonce","type":{"kind":"field"},"visibility":"private"},{"name":"wallet_address","type":{"kind":"field"},"visibility":"public"},{"name":"contract_address","type":{"kind":"field"},"visibility":"public"},{"name":"map_storage_slot","type":{"kind":"field"},"visibility":"public"},{"name":"note_hash_tree_root","type":{"kind":"field"},"visibility":"public"}],"return_type":null,"error_types":{"4471134686353999239":{"error_kind":"fmtstring","length":55,"item_types":[{"kind":"field"},{"kind":"field"}]},"12031443157092481308":{"error_kind":"string","string":"wallet address and note owner mismatch"},"12822839658937144934":{"error_kind":"fmtstring","length":75,"item_types":[]}}},"bytecode":"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","debug_symbols":"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","file_map":{"18":{"source":"pub mod bn254;\nuse crate::{runtime::is_unconstrained, static_assert};\nuse bn254::lt as bn254_lt;\n\nimpl Field {\n    /// Asserts that `self` can be represented in `bit_size` bits.\n    ///\n    /// # Failures\n    /// Causes a constraint failure for `Field` values exceeding `2^{bit_size}`.\n    // docs:start:assert_max_bit_size\n    pub fn assert_max_bit_size<let BIT_SIZE: u32>(self) {\n        // docs:end:assert_max_bit_size\n        static_assert(\n            BIT_SIZE < modulus_num_bits() as u32,\n            \"BIT_SIZE must be less than modulus_num_bits\",\n        );\n        __assert_max_bit_size(self, BIT_SIZE);\n    }\n\n    /// Decomposes `self` into its little endian bit decomposition as a `[u1; N]` array.\n    /// This slice will be zero padded should not all bits be necessary to represent `self`.\n    ///\n    /// # Failures\n    /// Causes a constraint failure for `Field` values exceeding `2^N` as the resulting slice will not\n    /// be able to represent the original `Field`.\n    ///\n    /// # Safety\n    /// The bit decomposition returned is canonical and is guaranteed to not overflow the modulus.\n    // docs:start:to_le_bits\n    pub fn to_le_bits<let N: u32>(self: Self) -> [u1; N] {\n        // docs:end:to_le_bits\n        let bits = __to_le_bits(self);\n\n        if !is_unconstrained() {\n            // Ensure that the byte decomposition does not overflow the modulus\n            let p = modulus_le_bits();\n            assert(bits.len() <= p.len());\n            let mut ok = bits.len() != p.len();\n            for i in 0..N {\n                if !ok {\n                    if (bits[N - 1 - i] != p[N - 1 - i]) {\n                        assert(p[N - 1 - i] == 1);\n                        ok = true;\n                    }\n                }\n            }\n            assert(ok);\n        }\n        bits\n    }\n\n    /// Decomposes `self` into its big endian bit decomposition as a `[u1; N]` array.\n    /// This array will be zero padded should not all bits be necessary to represent `self`.\n    ///\n    /// # Failures\n    /// Causes a constraint failure for `Field` values exceeding `2^N` as the resulting slice will not\n    /// be able to represent the original `Field`.\n    ///\n    /// # Safety\n    /// The bit decomposition returned is canonical and is guaranteed to not overflow the modulus.\n    // docs:start:to_be_bits\n    pub fn to_be_bits<let N: u32>(self: Self) -> [u1; N] {\n        // docs:end:to_be_bits\n        let bits = __to_be_bits(self);\n\n        if !is_unconstrained() {\n            // Ensure that the decomposition does not overflow the modulus\n            let p = modulus_be_bits();\n            assert(bits.len() <= p.len());\n            let mut ok = bits.len() != p.len();\n            for i in 0..N {\n                if !ok {\n                    if (bits[i] != p[i]) {\n                        assert(p[i] == 1);\n                        ok = true;\n                    }\n                }\n            }\n            assert(ok);\n        }\n        bits\n    }\n\n    /// Decomposes `self` into its little endian byte decomposition as a `[u8;N]` array\n    /// This array will be zero padded should not all bytes be necessary to represent `self`.\n    ///\n    /// # Failures\n    ///  The length N of the array must be big enough to contain all the bytes of the 'self',\n    ///  and no more than the number of bytes required to represent the field modulus\n    ///\n    /// # Safety\n    /// The result is ensured to be the canonical decomposition of the field element\n    // docs:start:to_le_bytes\n    pub fn to_le_bytes<let N: u32>(self: Self) -> [u8; N] {\n        // docs:end:to_le_bytes\n        static_assert(\n            N <= modulus_le_bytes().len(),\n            \"N must be less than or equal to modulus_le_bytes().len()\",\n        );\n        // Compute the byte decomposition\n        let bytes = self.to_le_radix(256);\n\n        if !is_unconstrained() {\n            // Ensure that the byte decomposition does not overflow the modulus\n            let p = modulus_le_bytes();\n            assert(bytes.len() <= p.len());\n            let mut ok = bytes.len() != p.len();\n            for i in 0..N {\n                if !ok {\n                    if (bytes[N - 1 - i] != p[N - 1 - i]) {\n                        assert(bytes[N - 1 - i] < p[N - 1 - i]);\n                        ok = true;\n                    }\n                }\n            }\n            assert(ok);\n        }\n        bytes\n    }\n\n    /// Decomposes `self` into its big endian byte decomposition as a `[u8;N]` array of length required to represent the field modulus\n    /// This array will be zero padded should not all bytes be necessary to represent `self`.\n    ///\n    /// # Failures\n    ///  The length N of the array must be big enough to contain all the bytes of the 'self',\n    ///  and no more than the number of bytes required to represent the field modulus\n    ///\n    /// # Safety\n    /// The result is ensured to be the canonical decomposition of the field element\n    // docs:start:to_be_bytes\n    pub fn to_be_bytes<let N: u32>(self: Self) -> [u8; N] {\n        // docs:end:to_be_bytes\n        static_assert(\n            N <= modulus_le_bytes().len(),\n            \"N must be less than or equal to modulus_le_bytes().len()\",\n        );\n        // Compute the byte decomposition\n        let bytes = self.to_be_radix(256);\n\n        if !is_unconstrained() {\n            // Ensure that the byte decomposition does not overflow the modulus\n            let p = modulus_be_bytes();\n            assert(bytes.len() <= p.len());\n            let mut ok = bytes.len() != p.len();\n            for i in 0..N {\n                if !ok {\n                    if (bytes[i] != p[i]) {\n                        assert(bytes[i] < p[i]);\n                        ok = true;\n                    }\n                }\n            }\n            assert(ok);\n        }\n        bytes\n    }\n\n    fn to_le_radix<let N: u32>(self: Self, radix: u32) -> [u8; N] {\n        // Brillig does not need an immediate radix\n        if !crate::runtime::is_unconstrained() {\n            static_assert(1 < radix, \"radix must be greater than 1\");\n            static_assert(radix <= 256, \"radix must be less than or equal to 256\");\n            static_assert(radix & (radix - 1) == 0, \"radix must be a power of 2\");\n        }\n        __to_le_radix(self, radix)\n    }\n\n    fn to_be_radix<let N: u32>(self: Self, radix: u32) -> [u8; N] {\n        // Brillig does not need an immediate radix\n        if !crate::runtime::is_unconstrained() {\n            static_assert(1 < radix, \"radix must be greater than 1\");\n            static_assert(radix <= 256, \"radix must be less than or equal to 256\");\n            static_assert(radix & (radix - 1) == 0, \"radix must be a power of 2\");\n        }\n        __to_be_radix(self, radix)\n    }\n\n    // Returns self to the power of the given exponent value.\n    // Caution: we assume the exponent fits into 32 bits\n    // using a bigger bit size impacts negatively the performance and should be done only if the exponent does not fit in 32 bits\n    pub fn pow_32(self, exponent: Field) -> Field {\n        let mut r: Field = 1;\n        let b: [u1; 32] = exponent.to_le_bits();\n\n        for i in 1..33 {\n            r *= r;\n            r = (b[32 - i] as Field) * (r * self) + (1 - b[32 - i] as Field) * r;\n        }\n        r\n    }\n\n    // Parity of (prime) Field element, i.e. sgn0(x mod p) = 0 if x `elem` {0, ..., p-1} is even, otherwise sgn0(x mod p) = 1.\n    pub fn sgn0(self) -> u1 {\n        self as u1\n    }\n\n    pub fn lt(self, another: Field) -> bool {\n        if crate::compat::is_bn254() {\n            bn254_lt(self, another)\n        } else {\n            lt_fallback(self, another)\n        }\n    }\n\n    /// Convert a little endian byte array to a field element.\n    /// If the provided byte array overflows the field modulus then the Field will silently wrap around.\n    pub fn from_le_bytes<let N: u32>(bytes: [u8; N]) -> Field {\n        static_assert(\n            N <= modulus_le_bytes().len(),\n            \"N must be less than or equal to modulus_le_bytes().len()\",\n        );\n        let mut v = 1;\n        let mut result = 0;\n\n        for i in 0..N {\n            result += (bytes[i] as Field) * v;\n            v = v * 256;\n        }\n        result\n    }\n\n    /// Convert a big endian byte array to a field element.\n    /// If the provided byte array overflows the field modulus then the Field will silently wrap around.\n    pub fn from_be_bytes<let N: u32>(bytes: [u8; N]) -> Field {\n        let mut v = 1;\n        let mut result = 0;\n\n        for i in 0..N {\n            result += (bytes[N - 1 - i] as Field) * v;\n            v = v * 256;\n        }\n        result\n    }\n}\n\n#[builtin(apply_range_constraint)]\nfn __assert_max_bit_size(value: Field, bit_size: u32) {}\n\n// `_radix` must be less than 256\n#[builtin(to_le_radix)]\nfn __to_le_radix<let N: u32>(value: Field, radix: u32) -> [u8; N] {}\n\n// `_radix` must be less than 256\n#[builtin(to_be_radix)]\nfn __to_be_radix<let N: u32>(value: Field, radix: u32) -> [u8; N] {}\n\n/// Decomposes `self` into its little endian bit decomposition as a `[u1; N]` array.\n/// This slice will be zero padded should not all bits be necessary to represent `self`.\n///\n/// # Failures\n/// Causes a constraint failure for `Field` values exceeding `2^N` as the resulting slice will not\n/// be able to represent the original `Field`.\n///\n/// # Safety\n/// Values of `N` equal to or greater than the number of bits necessary to represent the `Field` modulus\n/// (e.g. 254 for the BN254 field) allow for multiple bit decompositions. This is due to how the `Field` will\n/// wrap around due to overflow when verifying the decomposition.\n#[builtin(to_le_bits)]\nfn __to_le_bits<let N: u32>(value: Field) -> [u1; N] {}\n\n/// Decomposes `self` into its big endian bit decomposition as a `[u1; N]` array.\n/// This array will be zero padded should not all bits be necessary to represent `self`.\n///\n/// # Failures\n/// Causes a constraint failure for `Field` values exceeding `2^N` as the resulting slice will not\n/// be able to represent the original `Field`.\n///\n/// # Safety\n/// Values of `N` equal to or greater than the number of bits necessary to represent the `Field` modulus\n/// (e.g. 254 for the BN254 field) allow for multiple bit decompositions. This is due to how the `Field` will\n/// wrap around due to overflow when verifying the decomposition.\n#[builtin(to_be_bits)]\nfn __to_be_bits<let N: u32>(value: Field) -> [u1; N] {}\n\n#[builtin(modulus_num_bits)]\npub comptime fn modulus_num_bits() -> u64 {}\n\n#[builtin(modulus_be_bits)]\npub comptime fn modulus_be_bits() -> [u1] {}\n\n#[builtin(modulus_le_bits)]\npub comptime fn modulus_le_bits() -> [u1] {}\n\n#[builtin(modulus_be_bytes)]\npub comptime fn modulus_be_bytes() -> [u8] {}\n\n#[builtin(modulus_le_bytes)]\npub comptime fn modulus_le_bytes() -> [u8] {}\n\n/// An unconstrained only built in to efficiently compare fields.\n#[builtin(field_less_than)]\nunconstrained fn __field_less_than(x: Field, y: Field) -> bool {}\n\npub(crate) unconstrained fn field_less_than(x: Field, y: Field) -> bool {\n    __field_less_than(x, y)\n}\n\n// Convert a 32 byte array to a field element by modding\npub fn bytes32_to_field(bytes32: [u8; 32]) -> Field {\n    // Convert it to a field element\n    let mut v = 1;\n    let mut high = 0 as Field;\n    let mut low = 0 as Field;\n\n    for i in 0..16 {\n        high = high + (bytes32[15 - i] as Field) * v;\n        low = low + (bytes32[16 + 15 - i] as Field) * v;\n        v = v * 256;\n    }\n    // Abuse that a % p + b % p = (a + b) % p and that low < p\n    low + high * v\n}\n\nfn lt_fallback(x: Field, y: Field) -> bool {\n    if is_unconstrained() {\n        // Safety: unconstrained context\n        unsafe {\n            field_less_than(x, y)\n        }\n    } else {\n        let x_bytes: [u8; 32] = x.to_le_bytes();\n        let y_bytes: [u8; 32] = y.to_le_bytes();\n        let mut x_is_lt = false;\n        let mut done = false;\n        for i in 0..32 {\n            if (!done) {\n                let x_byte = x_bytes[32 - 1 - i] as u8;\n                let y_byte = y_bytes[32 - 1 - i] as u8;\n                let bytes_match = x_byte == y_byte;\n                if !bytes_match {\n                    x_is_lt = x_byte < y_byte;\n                    done = true;\n                }\n            }\n        }\n        x_is_lt\n    }\n}\n\nmod tests {\n    use crate::{panic::panic, runtime};\n    use super::field_less_than;\n\n    #[test]\n    // docs:start:to_be_bits_example\n    fn test_to_be_bits() {\n        let field = 2;\n        let bits: [u1; 8] = field.to_be_bits();\n        assert_eq(bits, [0, 0, 0, 0, 0, 0, 1, 0]);\n    }\n    // docs:end:to_be_bits_example\n\n    #[test]\n    // docs:start:to_le_bits_example\n    fn test_to_le_bits() {\n        let field = 2;\n        let bits: [u1; 8] = field.to_le_bits();\n        assert_eq(bits, [0, 1, 0, 0, 0, 0, 0, 0]);\n    }\n    // docs:end:to_le_bits_example\n\n    #[test]\n    // docs:start:to_be_bytes_example\n    fn test_to_be_bytes() {\n        let field = 2;\n        let bytes: [u8; 8] = field.to_be_bytes();\n        assert_eq(bytes, [0, 0, 0, 0, 0, 0, 0, 2]);\n        assert_eq(Field::from_be_bytes::<8>(bytes), field);\n    }\n    // docs:end:to_be_bytes_example\n\n    #[test]\n    // docs:start:to_le_bytes_example\n    fn test_to_le_bytes() {\n        let field = 2;\n        let bytes: [u8; 8] = field.to_le_bytes();\n        assert_eq(bytes, [2, 0, 0, 0, 0, 0, 0, 0]);\n        assert_eq(Field::from_le_bytes::<8>(bytes), field);\n    }\n    // docs:end:to_le_bytes_example\n\n    #[test]\n    // docs:start:to_be_radix_example\n    fn test_to_be_radix() {\n        // 259, in base 256, big endian, is [1, 3].\n        // i.e. 3 * 256^0 + 1 * 256^1\n        let field = 259;\n\n        // The radix (in this example, 256) must be a power of 2.\n        // The length of the returned byte array can be specified to be\n        // >= the amount of space needed.\n        let bytes: [u8; 8] = field.to_be_radix(256);\n        assert_eq(bytes, [0, 0, 0, 0, 0, 0, 1, 3]);\n        assert_eq(Field::from_be_bytes::<8>(bytes), field);\n    }\n    // docs:end:to_be_radix_example\n\n    #[test]\n    // docs:start:to_le_radix_example\n    fn test_to_le_radix() {\n        // 259, in base 256, little endian, is [3, 1].\n        // i.e. 3 * 256^0 + 1 * 256^1\n        let field = 259;\n\n        // The radix (in this example, 256) must be a power of 2.\n        // The length of the returned byte array can be specified to be\n        // >= the amount of space needed.\n        let bytes: [u8; 8] = field.to_le_radix(256);\n        assert_eq(bytes, [3, 1, 0, 0, 0, 0, 0, 0]);\n        assert_eq(Field::from_le_bytes::<8>(bytes), field);\n    }\n    // docs:end:to_le_radix_example\n\n    #[test(should_fail_with = \"radix must be greater than 1\")]\n    fn test_to_le_radix_1() {\n        // this test should only fail in constrained mode\n        if !runtime::is_unconstrained() {\n            let field = 2;\n            let _: [u8; 8] = field.to_le_radix(1);\n        } else {\n            panic(f\"radix must be greater than 1\");\n        }\n    }\n\n    // TODO: Update this test to account for the Brillig restriction that the radix must be greater than 2\n    //#[test]\n    //fn test_to_le_radix_brillig_1() {\n    //    // this test should only fail in constrained mode\n    //    if runtime::is_unconstrained() {\n    //        let field = 1;\n    //        let out: [u8; 8] = field.to_le_radix(1);\n    //        crate::println(out);\n    //        let expected = [0; 8];\n    //        assert(out == expected, \"unexpected result\");\n    //    }\n    //}\n\n    #[test(should_fail_with = \"radix must be a power of 2\")]\n    fn test_to_le_radix_3() {\n        // this test should only fail in constrained mode\n        if !runtime::is_unconstrained() {\n            let field = 2;\n            let _: [u8; 8] = field.to_le_radix(3);\n        } else {\n            panic(f\"radix must be a power of 2\");\n        }\n    }\n\n    #[test]\n    fn test_to_le_radix_brillig_3() {\n        // this test should only fail in constrained mode\n        if runtime::is_unconstrained() {\n            let field = 1;\n            let out: [u8; 8] = field.to_le_radix(3);\n            let mut expected = [0; 8];\n            expected[0] = 1;\n            assert(out == expected, \"unexpected result\");\n        }\n    }\n\n    #[test(should_fail_with = \"radix must be less than or equal to 256\")]\n    fn test_to_le_radix_512() {\n        // this test should only fail in constrained mode\n        if !runtime::is_unconstrained() {\n            let field = 2;\n            let _: [u8; 8] = field.to_le_radix(512);\n        } else {\n            panic(f\"radix must be less than or equal to 256\")\n        }\n    }\n\n    // TODO: Update this test to account for the Brillig restriction that the radix must be less than 512\n    //#[test]\n    //fn test_to_le_radix_brillig_512() {\n    //    // this test should only fail in constrained mode\n    //    if runtime::is_unconstrained() {\n    //        let field = 1;\n    //        let out: [u8; 8] = field.to_le_radix(512);\n    //        let mut expected = [0; 8];\n    //        expected[0] = 1;\n    //        assert(out == expected, \"unexpected result\");\n    //    }\n    //}\n\n    #[test]\n    unconstrained fn test_field_less_than() {\n        assert(field_less_than(0, 1));\n        assert(field_less_than(0, 0x100));\n        assert(field_less_than(0x100, 0 - 1));\n        assert(!field_less_than(0 - 1, 0));\n    }\n}\n","path":"std/field/mod.nr"},"43":{"source":"pub fn panic<T, U, let N: u32>(message: fmtstr<N, T>) -> U {\n    assert(false, message);\n    crate::mem::zeroed()\n}\n","path":"std/panic.nr"},"50":{"source":"mod nft_note;\n\nuse aztec::{\n    prelude::AztecAddress,\n    protocol_types::{\n        constants::NOTE_HASH_TREE_HEIGHT,\n        merkle_tree::membership::MembershipWitness,\n        storage::map::derive_storage_slot_in_map,\n        traits::{FromField, ToField},\n    },\n};\n\nuse nft_note::{NFTNote};\n\nfn main(\n    note: NFTNote,\n    // The membership witness for the note hash in the note hash tree.\n    membership_witness: MembershipWitness<NOTE_HASH_TREE_HEIGHT>,\n    // The nonce of the note for the note hash.\n    note_nonce: Field,\n    wallet_address: pub Field,\n    // The Aztec smart contract address.\n    contract_address: pub Field,\n    // The storage slot of the mapping in the contract.\n    map_storage_slot: pub Field,\n    // The expected value of the note.\n    // expected_value: pub u128,\n    // The real root of the note hash tree.\n    note_hash_tree_root: pub Field,\n) {\n    let real_value = note.get_token_id();\n    assert(wallet_address == note.owner.to_field(), \"wallet address and note owner mismatch\");\n    // assert(real_value == expected_value, f\"value mismatch: {real_value} != {expected_value}\");\n\n    let storage_slot = derive_storage_slot_in_map(map_storage_slot, note.owner.to_field());\n    storage_proofs::assert_note_inclusion(\n        note,\n        note_nonce,\n        membership_witness,\n        AztecAddress::from_field(contract_address),\n        storage_slot,\n        note_hash_tree_root,\n    );\n}\n","path":"/Users/satyam/projects/aztec-projects/raven-house/raven-house-app/circuits/example_circuit/src/main.nr"},"51":{"source":"use dep::aztec::{\n    context::{PrivateContext, PublicContext},\n    keys::getters::{get_nsk_app, get_public_keys},\n    macros::notes::custom_note,\n    messages::logs::note,\n    note::note_interface::{NoteHash, NoteType},\n    oracle::random::random,\n    protocol_types::{\n        address::AztecAddress,\n        constants::{GENERATOR_INDEX__NOTE_HASH, GENERATOR_INDEX__NOTE_NULLIFIER},\n        hash::poseidon2_hash_with_separator,\n        traits::{Deserialize, Hash, Packable, Serialize},\n        utils::arrays::array_concat,\n    },\n};\n\n// NFTNote supports partial notes, i.e. the ability to create an incomplete note in private, hiding certain values (the\n// owner, storage slot and randomness), and then completing the note in public with the ones missing (the token id).\n// Partial notes are being actively developed and are not currently fully supported via macros, and so we rely on the\n// #[custom_note] macro to implement it manually, resulting in some boilerplate. This is expected to be unnecessary once\n// macro support is expanded.\n\n// docs:start:nft_note\n/// A private note representing a token id associated to an account.\n#[custom_note]\n#[derive(Eq, Serialize)]\npub struct NFTNote {\n    // The ordering of these fields is important given that it must:\n    //   a) match that of NFTPartialNotePrivateContent, and\n    //   b) have the public field at the end\n    // Correct ordering is checked by the tests in this module.\n\n    /// The owner of the note, i.e. the account whose nullifier secret key is required to compute the nullifier.\n    owner: AztecAddress,\n    /// Random value, protects against note hash preimage attacks.\n    randomness: Field,\n    /// The ID of the token represented by this note.\n    token_id: Field,\n}\n// docs:end:nft_note\n\nimpl NoteHash for NFTNote {\n    fn compute_note_hash(self, storage_slot: Field) -> Field {\n        // Partial notes can be implemented by having the note hash be either the result of multiscalar multiplication\n        // (MSM), or two rounds of poseidon. MSM results in more constraints and is only required when multiple variants\n        // of partial notes are supported. Because NFTNote has just one variant (where the token id is public), we use\n        // poseidon instead.\n\n        // We must compute the same note hash as would be produced by a partial note created and completed with the same\n        // values, so that notes all behave the same way regardless of how they were created. To achieve this, we\n        // perform both steps of the partial note computation.\n\n        // First we create the partial note from a commitment to the private content (including storage slot).\n        let private_content =\n            NFTPartialNotePrivateContent { owner: self.owner, randomness: self.randomness };\n        let partial_note =\n            PartialNFTNote { commitment: private_content.compute_partial_commitment(storage_slot) };\n\n        // Then compute the completion note hash. In a real partial note this step would be performed in public.\n        partial_note.compute_complete_note_hash(self.token_id)\n    }\n\n    // The nullifiers are nothing special - this is just the canonical implementation that would be injected by the\n    // #[note] macro.\n\n    fn compute_nullifier(\n        self,\n        context: &mut PrivateContext,\n        note_hash_for_nullify: Field,\n    ) -> Field {\n        let owner_npk_m = get_public_keys(self.owner).npk_m;\n        let owner_npk_m_hash = owner_npk_m.hash();\n        let secret = context.request_nsk_app(owner_npk_m_hash);\n        poseidon2_hash_with_separator(\n            [note_hash_for_nullify, secret],\n            GENERATOR_INDEX__NOTE_NULLIFIER,\n        )\n    }\n\n    unconstrained fn compute_nullifier_unconstrained(self, note_hash_for_nullify: Field) -> Field {\n        let owner_npk_m = get_public_keys(self.owner).npk_m;\n        let owner_npk_m_hash = owner_npk_m.hash();\n        let secret = get_nsk_app(owner_npk_m_hash);\n        poseidon2_hash_with_separator(\n            [note_hash_for_nullify, secret],\n            GENERATOR_INDEX__NOTE_NULLIFIER,\n        )\n    }\n}\n\nimpl NFTNote {\n    pub fn new(token_id: Field, owner: AztecAddress) -> Self {\n        // Safety: We use the randomness to preserve the privacy of the note recipient by preventing brute-forcing,\n        // so a malicious sender could use non-random values to make the note less private. But they already know\n        // the full note pre-image anyway, and so the recipient already trusts them to not disclose this\n        // information. We can therefore assume that the sender will cooperate in the random value generation.\n        let randomness = unsafe { random() };\n        Self { token_id, owner, randomness }\n    }\n\n    pub fn get_token_id(self) -> Field {\n        self.token_id\n    }\n\n    /// Creates a partial note that will hide the owner and storage slot but not the token id, since the note will be\n    /// later completed in public. This is a powerful technique for scenarios in which the token id cannot be known in\n    /// private (e.g. because it depends on some public state, such as a DEX).\n    ///\n    /// The returned `PartialNFTNote` value must be sent to public execution via a secure channel, since it is not\n    /// possible to verify the integrity of its contents due to it hiding information. The recommended ways to do this\n    /// are to retrieve it from public storage, or to receive it in an internal public function call.\n    ///\n    /// Each partial note should only be used once, since otherwise multiple notes would be linked together and known to\n    /// belong to the same owner.\n    ///\n    /// As part of the partial note creation process, a log will be sent to `recipient` from `sender` so that they can\n    /// discover the note. `recipient` will typically be the same as `owner`.\n    pub fn partial(\n        owner: AztecAddress,\n        storage_slot: Field,\n        context: &mut PrivateContext,\n        recipient: AztecAddress,\n        sender: AztecAddress,\n    ) -> PartialNFTNote {\n        // Safety: We use the randomness to preserve the privacy of the note recipient by preventing brute-forcing,\n        // so a malicious sender could use non-random values to make the note less private. But they already know\n        // the full note pre-image anyway, and so the recipient already trusts them to not disclose this\n        // information. We can therefore assume that the sender will cooperate in the random value generation.\n        let randomness = unsafe { random() };\n\n        // We create a commitment to the private data, which we then use to construct the log we send to the recipient.\n        let commitment = NFTPartialNotePrivateContent { owner, randomness }\n            .compute_partial_commitment(storage_slot);\n\n        // Our partial note log encoding scheme includes a field with the tag of the public completion log, and we use\n        // the commitment as the tag. This is good for multiple reasons:\n        //  - the commitment is uniquely tied to this partial note\n        //  - the commitment is already public information, so we're not revealing anything else\n        //  - we don't need to create any additional information, private or public, for the tag\n        //  - other contracts cannot impersonate us and emit logs with the same tag due to public log siloing\n        let private_log_content = PrivateNFTPartialNotePrivateLogContent {\n            owner,\n            randomness,\n            public_log_tag: commitment,\n        };\n\n        let encrypted_log =\n            note::compute_partial_note_log(private_log_content, storage_slot, recipient, sender);\n        let length = encrypted_log.len();\n        context.emit_private_log(encrypted_log, length);\n\n        PartialNFTNote { commitment }\n    }\n}\n\n/// The private content of a partial NFTNote, i.e. the fields that will remain private. All other note fields will be\n/// made public.\n#[derive(Packable)]\nstruct NFTPartialNotePrivateContent {\n    // The ordering of these fields is important given that it must match that of NFTNote.\n    // Correct ordering is checked by the tests in this module.\n    owner: AztecAddress,\n    randomness: Field,\n}\n\nimpl NFTPartialNotePrivateContent {\n    fn compute_partial_commitment(self, storage_slot: Field) -> Field {\n        // Here we commit to all private values, including the storage slot.\n        poseidon2_hash_with_separator(\n            array_concat(self.pack(), [storage_slot]),\n            GENERATOR_INDEX__NOTE_HASH,\n        )\n    }\n}\n\n#[derive(Packable)]\nstruct PrivateNFTPartialNotePrivateLogContent {\n    // The ordering of these fields is important given that it must:\n    //   a) match that of NFTNote, and\n    //   b) have the public log tag at the beginning\n    // Correct ordering is checked by the tests in this module.\n    public_log_tag: Field,\n    owner: AztecAddress,\n    randomness: Field,\n}\n\nimpl NoteType for PrivateNFTPartialNotePrivateLogContent {\n    fn get_id() -> Field {\n        NFTNote::get_id()\n    }\n}\n\n/// A partial instance of a NFTNote. This value represents a private commitment to the owner, randomness and storage\n/// slot, but the token id field has not yet been set. A partial note can be completed in public with the `complete`\n/// function (revealing the token id to the public), resulting in a NFTNote that can be used like any other one (except\n/// of course that its token id is known).\n#[derive(Packable, Serialize, Deserialize)]\npub struct PartialNFTNote {\n    commitment: Field,\n}\n\nimpl PartialNFTNote {\n    pub fn commitment(self) -> Field {\n        self.commitment\n    }\n}\n\nimpl PartialNFTNote {\n    /// Completes the partial note, creating a new note that can be used like any other NFTNote.\n    pub fn complete(self, token_id: Field, context: &mut PublicContext) {\n        // A note with a value of zero is valid, but we cannot currently complete a partial note with such a value\n        // because this will result in the completion log having its last field set to 0. Public logs currently do not\n        // track their length, and so trailing zeros are simply trimmed. This results in the completion log missing its\n        // last field (the value), and note discovery failing.\n        // TODO(#11636): remove this\n        assert(token_id != 0, \"Cannot complete a PartialNFTNote with a value of 0\");\n\n        // We need to do two things:\n        //  - emit a public log containing the public fields (the token id). The contract will later find it by\n        //  searching for the expected tag (which is simply the partial note commitment).\n        //  - insert the completion note hash (i.e. the hash of the note) into the note hash tree. This is typically\n        //  only done in private to hide the preimage of the hash that is inserted, but completed partial notes are\n        //  inserted in public as the public values are provided and the note hash computed.\n        context.emit_public_log(self.compute_note_completion_log(token_id));\n        context.push_note_hash(self.compute_complete_note_hash(token_id));\n    }\n\n    fn compute_note_completion_log(self, token_id: Field) -> [Field; 2] {\n        // The first field of this log must be the tag that the recipient of the partial note private field logs\n        // expects, which is equal to the partial note commitment.\n        [self.commitment, token_id]\n    }\n\n    fn compute_complete_note_hash(self, token_id: Field) -> Field {\n        // Here we finalize the note hash by including the (public) token id into the partial note commitment. Note that\n        // we use the same generator index as we used for the first round of poseidon - this is not an issue.\n        poseidon2_hash_with_separator([self.commitment, token_id], GENERATOR_INDEX__NOTE_HASH)\n    }\n}\n\nmod test {\n    use super::{\n        NFTNote, NFTPartialNotePrivateContent, PartialNFTNote,\n        PrivateNFTPartialNotePrivateLogContent,\n    };\n    use dep::aztec::{\n        note::note_interface::NoteHash,\n        protocol_types::{\n            address::AztecAddress,\n            traits::{FromField, Packable},\n            utils::arrays::array_concat,\n        },\n        utils::array::subarray,\n    };\n\n    global token_id: Field = 17;\n    global randomness: Field = 42;\n    global owner: AztecAddress = AztecAddress::from_field(50);\n    global storage_slot: Field = 13;\n\n    #[test]\n    fn note_hash_matches_completed_partial_note_hash() {\n        // Tests that a NFTNote has the same note hash as a PartialNFTNote created and then completed with the same\n        // private values. This requires for the same hash function to be used in both flows, with the fields in the\n        // same order.\n\n        let note = NFTNote { token_id, randomness, owner };\n        let note_hash = note.compute_note_hash(storage_slot);\n\n        let partial_note_private_content = NFTPartialNotePrivateContent { owner, randomness };\n\n        let partial_note = PartialNFTNote {\n            commitment: partial_note_private_content.compute_partial_commitment(storage_slot),\n        };\n        let completed_partial_note_hash = partial_note.compute_complete_note_hash(token_id);\n\n        assert_eq(note_hash, completed_partial_note_hash);\n    }\n\n    #[test]\n    fn unpack_from_partial_note_encoding() {\n        // Tests that the packed representation of a regular NFTNote can be reconstructed given the partial note\n        // private fields log and the public completion log, ensuring the recipient will be able to compute the\n        // completed note as if it were a regular NFTNote.\n\n        let note = NFTNote { token_id, randomness, owner };\n\n        let partial_note_private_content = NFTPartialNotePrivateContent { owner, randomness };\n        let commitment = partial_note_private_content.compute_partial_commitment(storage_slot);\n\n        let private_log_content = PrivateNFTPartialNotePrivateLogContent {\n            owner,\n            randomness,\n            public_log_tag: commitment,\n        };\n        let partial_note = PartialNFTNote { commitment };\n\n        // The first field of the partial note private content is the public completion log tag, so it should match the\n        // first field of the public log.\n        assert_eq(\n            private_log_content.pack()[0],\n            partial_note.compute_note_completion_log(token_id)[0],\n        );\n\n        // Then we extract all fields except the first of both logs (i.e. the public log tag), and combine them to\n        // produce the note's packed representation. This requires that the members of the intermediate structs are in\n        // the same order as in NFTNote.\n        let private_log_without_public_tag: [_; 2] = subarray(private_log_content.pack(), 1);\n        let public_log_without_tag: [_; 1] =\n            subarray(partial_note.compute_note_completion_log(token_id), 1);\n\n        assert_eq(\n            array_concat(private_log_without_public_tag, public_log_without_tag),\n            note.pack(),\n        );\n    }\n}\n","path":"/Users/satyam/projects/aztec-projects/raven-house/raven-house-app/circuits/example_circuit/src/nft_note.nr"},"148":{"source":"use protocol_types::traits::Serialize;\n\n// There's temporarily quite a bit of boilerplate here because Noir does not yet support enums. This file will\n// eventually be simplified into something closer to:\n//\n// pub enum NoteMetadata {\n//   PendingSamePhase{ note_hash_counter: u32 },\n//   PendingOtherPhase{ note_hash_counter: u32, note_nonce: Field },\n//   Settled{ note_nonce: Field },\n// }\n//\n// For now, we have `NoteMetadata` acting as a sort of tagged union.\n\nstruct NoteStageEnum {\n    /// A note that was created in the transaction that is currently being executed, during the current execution phase,\n    /// i.e. non-revertible or revertible.\n    ///\n    /// These notes are not yet in the note hash tree, though they will be inserted unless nullified in this transaction\n    /// (becoming a transient note).\n    PENDING_SAME_PHASE: u8,\n    /// A note that was created in the transaction that is currently being executed, during the previous execution\n    /// phase. Because there are only two phases and their order is always the same (first non-revertible and then\n    /// revertible) this implies that the note was created in the non-revertible phase, and that the current phase is\n    /// the revertible phase.\n    ///\n    /// These notes are not yet in the note hash tree, though they will be inserted **even if nullified in this\n    /// transaction**. This means that they must be nullified as if they were settled (i.e. using the unique note hash)\n    /// in order to avoid double spends once they become settled.\n    PENDING_PREVIOUS_PHASE: u8,\n    /// A note that was created in a prior transaction and is therefore already in the note hash tree.\n    SETTLED: u8,\n}\n\nglobal NoteStage: NoteStageEnum =\n    NoteStageEnum { PENDING_SAME_PHASE: 1, PENDING_PREVIOUS_PHASE: 2, SETTLED: 3 };\n\n/// The metadata required to both prove a note's existence and destroy it, by computing the correct note hash for kernel\n/// read requests, as well as the correct nullifier to avoid double-spends.\n///\n/// This represents a note in any of the three valid stages (pending same phase, pending previous phase, or settled). In\n/// order to access the underlying fields callers must first find the appropriate stage (e.g. via `is_settled()`) and\n/// then convert this into the appropriate type (e.g. via `to_settled()`).\n#[derive(Eq, Serialize)]\npub struct NoteMetadata {\n    stage: u8,\n    maybe_note_nonce: Field,\n}\n\nimpl NoteMetadata {\n    /// Constructs a `NoteMetadata` object from optional note hash counter and nonce. Both a zero note hash counter and\n    /// a zero nonce are invalid, so those are used to signal non-existent values.\n    pub fn from_raw_data(nonzero_note_hash_counter: bool, maybe_note_nonce: Field) -> Self {\n        if nonzero_note_hash_counter {\n            if maybe_note_nonce == 0 {\n                Self { stage: NoteStage.PENDING_SAME_PHASE, maybe_note_nonce }\n            } else {\n                Self { stage: NoteStage.PENDING_PREVIOUS_PHASE, maybe_note_nonce }\n            }\n        } else if maybe_note_nonce != 0 {\n            Self { stage: NoteStage.SETTLED, maybe_note_nonce }\n        } else {\n            panic(\n                f\"Note has a zero note hash counter and no nonce - existence cannot be proven\",\n            )\n        }\n    }\n\n    /// Returns true if the note is pending **and** from the same phase, i.e. if it's been created in the current\n    /// transaction during the current execution phase (either non-revertible or revertible).\n    pub fn is_pending_same_phase(self) -> bool {\n        self.stage == NoteStage.PENDING_SAME_PHASE\n    }\n\n    /// Returns true if the note is pending **and** from the previous phase, i.e. if it's been created in the current\n    /// transaction during an execution phase prior to the current one. Because private execution only has two phases\n    /// with strict ordering, this implies that the note was created in the non-revertible phase, and that the current\n    /// phase is the revertible phase.\n    pub fn is_pending_previous_phase(self) -> bool {\n        self.stage == NoteStage.PENDING_PREVIOUS_PHASE\n    }\n\n    /// Returns true if the note is settled, i.e. if it's been created in a prior transaction and is therefore already\n    /// in the note hash tree.\n    pub fn is_settled(self) -> bool {\n        self.stage == NoteStage.SETTLED\n    }\n\n    /// Asserts that the metadata is that of a pending note from the same phase and converts it accordingly.\n    pub fn to_pending_same_phase(self) -> PendingSamePhaseNoteMetadata {\n        assert_eq(self.stage, NoteStage.PENDING_SAME_PHASE);\n        PendingSamePhaseNoteMetadata::new()\n    }\n\n    /// Asserts that the metadata is that of a pending note from a previous phase and converts it accordingly.\n    pub fn to_pending_previous_phase(self) -> PendingPreviousPhaseNoteMetadata {\n        assert_eq(self.stage, NoteStage.PENDING_PREVIOUS_PHASE);\n        PendingPreviousPhaseNoteMetadata::new(self.maybe_note_nonce)\n    }\n\n    /// Asserts that the metadata is that of a settled note and converts it accordingly.\n    pub fn to_settled(self) -> SettledNoteMetadata {\n        assert_eq(self.stage, NoteStage.SETTLED);\n        SettledNoteMetadata::new(self.maybe_note_nonce)\n    }\n}\n\nimpl From<PendingSamePhaseNoteMetadata> for NoteMetadata {\n    fn from(_value: PendingSamePhaseNoteMetadata) -> Self {\n        NoteMetadata::from_raw_data(true, std::mem::zeroed())\n    }\n}\n\nimpl From<PendingPreviousPhaseNoteMetadata> for NoteMetadata {\n    fn from(value: PendingPreviousPhaseNoteMetadata) -> Self {\n        NoteMetadata::from_raw_data(true, value.note_nonce())\n    }\n}\n\nimpl From<SettledNoteMetadata> for NoteMetadata {\n    fn from(value: SettledNoteMetadata) -> Self {\n        NoteMetadata::from_raw_data(false, value.note_nonce())\n    }\n}\n\n/// The metadata required to both prove a note's existence and destroy it, by computing the correct note hash for kernel\n/// read requests, as well as the correct nullifier to avoid double-spends.\n///\n/// This represents a pending same phase note, i.e. a note that was created in the transaction that is currently being\n/// executed during the current execution phase (either non-revertible or revertible).\npub struct PendingSamePhaseNoteMetadata {\n    // This struct contains no fields since there is no metadata associated with a pending same phase note: it has no\n    // nonce (since it may get squashed by a nullifier emitted in the same phase), and while it does have a note hash\n    // counter we cannot constrain its value (and don't need to - only that it is non-zero).\n}\n\nimpl PendingSamePhaseNoteMetadata {\n    pub fn new() -> Self {\n        Self {}\n    }\n}\n\n/// The metadata required to both prove a note's existence and destroy it, by computing the correct note hash for kernel\n/// read requests, as well as the correct nullifier to avoid double-spends.\n///\n/// This represents a pending previous phase note, i.e. a note that was created in the transaction that is currently\n/// being executed, during the previous execution phase. Because there are only two phases and their order is always the\n/// same (first non-revertible and then revertible) this implies that the note was created in the non-revertible phase,\n/// and that the current phase is the revertible phase.\npub struct PendingPreviousPhaseNoteMetadata {\n    note_nonce: Field,\n    // This struct does not contain a note hash counter, even though one exists for this note, because we cannot\n    // constrain its value (and don't need to - only that it is non-zero).\n}\n\nimpl PendingPreviousPhaseNoteMetadata {\n    pub fn new(note_nonce: Field) -> Self {\n        Self { note_nonce }\n    }\n\n    pub fn note_nonce(self) -> Field {\n        self.note_nonce\n    }\n}\n\n/// The metadata required to both prove a note's existence and destroy it, by computing the correct note hash for kernel\n/// read requests, as well as the correct nullifier to avoid double-spends.\n///\n/// This represents a settled note, i.e. a note that was created in a prior transaction and is therefore already in the\n/// note hash tree.\npub struct SettledNoteMetadata {\n    note_nonce: Field,\n}\n\nimpl SettledNoteMetadata {\n    pub fn new(note_nonce: Field) -> Self {\n        Self { note_nonce }\n    }\n\n    pub fn note_nonce(self) -> Field {\n        self.note_nonce\n    }\n}\n","path":"/Users/satyam/nargo/github.com/AztecProtocol/aztec-packages/v1.1.2/noir-projects/aztec-nr/aztec/src/note/note_metadata.nr"},"151":{"source":"use crate::{\n    context::PrivateContext,\n    note::{note_interface::NoteHash, retrieved_note::RetrievedNote},\n};\n\nuse dep::protocol_types::hash::{\n    compute_siloed_note_hash, compute_siloed_nullifier, compute_unique_note_hash,\n};\n\n/// Returns the note hash that must be used to issue a private kernel read request for a note.\npub fn compute_note_hash_for_read_request<Note>(\n    retrieved_note: RetrievedNote<Note>,\n    storage_slot: Field,\n) -> Field\nwhere\n    Note: NoteHash,\n{\n    let note_hash = retrieved_note.note.compute_note_hash(storage_slot);\n\n    if retrieved_note.metadata.is_settled() {\n        // Settled notes are read by siloing with contract address and nonce (resulting in the final unique note hash,\n        // which is already in the note hash tree).\n        let siloed_note_hash = compute_siloed_note_hash(retrieved_note.contract_address, note_hash);\n        compute_unique_note_hash(\n            retrieved_note.metadata.to_settled().note_nonce(),\n            siloed_note_hash,\n        )\n    } else {\n        // Pending notes (both same phase and previous phase ones)  re read by their non-siloed hash (not even by\n        // contract address), which is what is stored in the new note hashes array (at the position hinted by note hash\n        // counter).\n        note_hash\n    }\n}\n\n/// Returns the note hash that must be used to compute a note's nullifier when calling `NoteHash::compute_nullifier` or\n/// `NoteHash::compute_nullifier_unconstrained`.\npub fn compute_note_hash_for_nullify<Note>(\n    retrieved_note: RetrievedNote<Note>,\n    storage_slot: Field,\n) -> Field\nwhere\n    Note: NoteHash,\n{\n    compute_note_hash_for_nullify_from_read_request(\n        retrieved_note,\n        compute_note_hash_for_read_request(retrieved_note, storage_slot),\n    )\n}\n\n/// Same as `compute_note_hash_for_nullify`, except it takes the note hash used in a read request (i.e. what\n/// `compute_note_hash_for_read_request` would return). This is useful in scenarios where that hash has already been\n/// computed to reduce constraints by reusing this value.\npub fn compute_note_hash_for_nullify_from_read_request<Note>(\n    retrieved_note: RetrievedNote<Note>,\n    note_hash_for_read_request: Field,\n) -> Field {\n    // There is just one instance in which the note hash for nullification does not match the note hash used for a read\n    // request, which is when dealing with pending previous phase notes. These had their existence proven using their\n    // non-siloed note hash along with the note hash counter (like all pending notes), but since they will be\n    // unconditionally inserted in the note hash tree (since they cannot be squashed) they must be nullified using the\n    // *unique* note hash.\n    // If we didn't, it'd be possible to emit a second different nullifier for the same note in a follow up transaction,\n    // once the note is settled, resulting in a double spend.\n\n    if retrieved_note.metadata.is_pending_previous_phase() {\n        let siloed_note_hash =\n            compute_siloed_note_hash(retrieved_note.contract_address, note_hash_for_read_request);\n        let note_nonce = retrieved_note.metadata.to_pending_previous_phase().note_nonce();\n\n        compute_unique_note_hash(note_nonce, siloed_note_hash)\n    } else {\n        note_hash_for_read_request\n    }\n}\n\n/// Computes a note's siloed nullifier, i.e. the one that will be inserted into the nullifier tree.\npub fn compute_siloed_note_nullifier<Note>(\n    retrieved_note: RetrievedNote<Note>,\n    storage_slot: Field,\n    context: &mut PrivateContext,\n) -> Field\nwhere\n    Note: NoteHash,\n{\n    let note_hash_for_nullify = compute_note_hash_for_nullify(retrieved_note, storage_slot);\n    let inner_nullifier = retrieved_note.note.compute_nullifier(context, note_hash_for_nullify);\n\n    compute_siloed_nullifier(retrieved_note.contract_address, inner_nullifier)\n}\n","path":"/Users/satyam/nargo/github.com/AztecProtocol/aztec-packages/v1.1.2/noir-projects/aztec-nr/aztec/src/note/utils.nr"},"223":{"source":"use std::default::Default;\nuse std::hash::Hasher;\n\ncomptime global RATE: u32 = 3;\n\npub struct Poseidon2 {\n    cache: [Field; 3],\n    state: [Field; 4],\n    cache_size: u32,\n    squeeze_mode: bool, // 0 => absorb, 1 => squeeze\n}\n\nimpl Poseidon2 {\n    #[no_predicates]\n    pub fn hash<let N: u32>(input: [Field; N], message_size: u32) -> Field {\n        Poseidon2::hash_internal(input, message_size, message_size != N)\n    }\n\n    pub(crate) fn new(iv: Field) -> Poseidon2 {\n        let mut result =\n            Poseidon2 { cache: [0; 3], state: [0; 4], cache_size: 0, squeeze_mode: false };\n        result.state[RATE] = iv;\n        result\n    }\n\n    fn perform_duplex(&mut self) {\n        // add the cache into sponge state\n        for i in 0..RATE {\n            // We effectively zero-pad the cache by only adding to the state\n            // cache that is less than the specified `cache_size`\n            if i < self.cache_size {\n                self.state[i] += self.cache[i];\n            }\n        }\n        self.state = crate::poseidon2_permutation(self.state, 4);\n    }\n\n    fn absorb(&mut self, input: Field) {\n        assert(!self.squeeze_mode);\n        if self.cache_size == RATE {\n            // If we're absorbing, and the cache is full, apply the sponge permutation to compress the cache\n            self.perform_duplex();\n            self.cache[0] = input;\n            self.cache_size = 1;\n        } else {\n            // If we're absorbing, and the cache is not full, add the input into the cache\n            self.cache[self.cache_size] = input;\n            self.cache_size += 1;\n        }\n    }\n\n    fn squeeze(&mut self) -> Field {\n        assert(!self.squeeze_mode);\n        // If we're in absorb mode, apply sponge permutation to compress the cache.\n        self.perform_duplex();\n        self.squeeze_mode = true;\n\n        // Pop one item off the top of the permutation and return it.\n        self.state[0]\n    }\n\n    fn hash_internal<let N: u32>(\n        input: [Field; N],\n        in_len: u32,\n        is_variable_length: bool,\n    ) -> Field {\n        let two_pow_64 = 18446744073709551616;\n        let iv: Field = (in_len as Field) * two_pow_64;\n        let mut sponge = Poseidon2::new(iv);\n        for i in 0..input.len() {\n            if i < in_len {\n                sponge.absorb(input[i]);\n            }\n        }\n\n        // In the case where the hash preimage is variable-length, we append `1` to the end of the input, to distinguish\n        // from fixed-length hashes. (the combination of this additional field element + the hash IV ensures\n        // fixed-length and variable-length hashes do not collide)\n        if is_variable_length {\n            sponge.absorb(1);\n        }\n        sponge.squeeze()\n    }\n}\n\npub struct Poseidon2Hasher {\n    _state: [Field],\n}\n\nimpl Hasher for Poseidon2Hasher {\n    fn finish(self) -> Field {\n        let iv: Field = (self._state.len() as Field) * 18446744073709551616; // iv = (self._state.len() << 64)\n        let mut sponge = Poseidon2::new(iv);\n        for i in 0..self._state.len() {\n            sponge.absorb(self._state[i]);\n        }\n        sponge.squeeze()\n    }\n\n    fn write(&mut self, input: Field) {\n        self._state = self._state.push_back(input);\n    }\n}\n\nimpl Default for Poseidon2Hasher {\n    fn default() -> Self {\n        Poseidon2Hasher { _state: &[] }\n    }\n}\n","path":"/Users/satyam/nargo/github.com/noir-lang/poseidon/v0.1.1/src/poseidon2.nr"},"302":{"source":"use crate::{\n    abis::{\n        contract_class_function_leaf_preimage::ContractClassFunctionLeafPreimage,\n        function_selector::FunctionSelector,\n        note_hash::ScopedNoteHash,\n        nullifier::ScopedNullifier,\n        private_log::{PrivateLog, PrivateLogData},\n        side_effect::{OrderedValue, scoped::Scoped},\n    },\n    address::{AztecAddress, EthAddress},\n    constants::{\n        CONTRACT_CLASS_LOG_SIZE_IN_FIELDS, FUNCTION_TREE_HEIGHT, GENERATOR_INDEX__NOTE_HASH_NONCE,\n        GENERATOR_INDEX__OUTER_NULLIFIER, GENERATOR_INDEX__SILOED_NOTE_HASH,\n        GENERATOR_INDEX__UNIQUE_NOTE_HASH, TWO_POW_64,\n    },\n    merkle_tree::root::root_from_sibling_path,\n    messaging::l2_to_l1_message::L2ToL1Message,\n    poseidon2::Poseidon2Sponge,\n    traits::{FromField, Hash, ToField},\n    utils::{arrays::array_concat, field::{field_from_bytes, field_from_bytes_32_trunc}},\n};\nuse std::embedded_curve_ops::EmbeddedCurveScalar;\n\npub fn sha256_to_field<let N: u32>(bytes_to_hash: [u8; N]) -> Field {\n    let sha256_hashed = sha256::digest(bytes_to_hash);\n    let hash_in_a_field = field_from_bytes_32_trunc(sha256_hashed);\n\n    hash_in_a_field\n}\n\npub fn private_functions_root_from_siblings(\n    selector: FunctionSelector,\n    vk_hash: Field,\n    function_leaf_index: Field,\n    function_leaf_sibling_path: [Field; FUNCTION_TREE_HEIGHT],\n) -> Field {\n    let function_leaf_preimage = ContractClassFunctionLeafPreimage { selector, vk_hash };\n    let function_leaf = function_leaf_preimage.hash();\n    root_from_sibling_path(\n        function_leaf,\n        function_leaf_index,\n        function_leaf_sibling_path,\n    )\n}\n\npub fn compute_note_hash_nonce(first_nullifier_in_tx: Field, note_index_in_tx: u32) -> Field {\n    // Hashing the first nullifier with note index in tx is guaranteed to be unique (because all nullifiers are also\n    // unique).\n    poseidon2_hash_with_separator(\n        [first_nullifier_in_tx, note_index_in_tx as Field],\n        GENERATOR_INDEX__NOTE_HASH_NONCE,\n    )\n}\n\npub fn compute_unique_note_hash(note_nonce: Field, siloed_note_hash: Field) -> Field {\n    let inputs = [note_nonce, siloed_note_hash];\n    poseidon2_hash_with_separator(inputs, GENERATOR_INDEX__UNIQUE_NOTE_HASH)\n}\n\npub fn compute_nonce_and_unique_note_hash(\n    siloed_note_hash: Field,\n    first_nullifier: Field,\n    note_index_in_tx: u32,\n) -> Field {\n    let note_nonce = compute_note_hash_nonce(first_nullifier, note_index_in_tx);\n    compute_unique_note_hash(note_nonce, siloed_note_hash)\n}\n\npub fn compute_siloed_note_hash(app: AztecAddress, note_hash: Field) -> Field {\n    poseidon2_hash_with_separator(\n        [app.to_field(), note_hash],\n        GENERATOR_INDEX__SILOED_NOTE_HASH,\n    )\n}\n\n/// Computes unique note hashes from siloed note hashes\npub fn compute_unique_siloed_note_hash(\n    siloed_note_hash: Field,\n    first_nullifier: Field,\n    note_index_in_tx: u32,\n) -> Field {\n    if siloed_note_hash == 0 {\n        0\n    } else {\n        compute_nonce_and_unique_note_hash(siloed_note_hash, first_nullifier, note_index_in_tx)\n    }\n}\n\n/// Siloing in the context of Aztec refers to the process of hashing a note hash with a contract address (this way\n/// the note hash is scoped to a specific contract). This is used to prevent intermingling of notes between contracts.\npub fn silo_note_hash(note_hash: ScopedNoteHash) -> Field {\n    if note_hash.contract_address.is_zero() {\n        0\n    } else {\n        compute_siloed_note_hash(note_hash.contract_address, note_hash.value())\n    }\n}\n\npub fn compute_siloed_nullifier(app: AztecAddress, nullifier: Field) -> Field {\n    poseidon2_hash_with_separator(\n        [app.to_field(), nullifier],\n        GENERATOR_INDEX__OUTER_NULLIFIER,\n    )\n}\n\npub fn silo_nullifier(nullifier: ScopedNullifier) -> Field {\n    if nullifier.contract_address.is_zero() {\n        nullifier.value() // Return value instead of 0 because the first nullifier's contract address is zero.\n    } else {\n        compute_siloed_nullifier(nullifier.contract_address, nullifier.value())\n    }\n}\n\npub fn compute_siloed_private_log_field(contract_address: AztecAddress, field: Field) -> Field {\n    poseidon2_hash([contract_address.to_field(), field])\n}\n\npub fn silo_private_log(private_log: Scoped<PrivateLogData>) -> PrivateLog {\n    if private_log.contract_address.is_zero() {\n        private_log.inner.log\n    } else {\n        let mut fields = private_log.inner.log.fields;\n        fields[0] = compute_siloed_private_log_field(private_log.contract_address, fields[0]);\n        PrivateLog::new(fields, private_log.inner.log.length)\n    }\n}\n\npub fn compute_contract_class_log_hash(log: [Field; CONTRACT_CLASS_LOG_SIZE_IN_FIELDS]) -> Field {\n    poseidon2_hash(log)\n}\n\npub fn compute_app_secret_key(\n    master_secret_key: EmbeddedCurveScalar,\n    app_address: AztecAddress,\n    app_secret_generator: Field,\n) -> Field {\n    poseidon2_hash_with_separator(\n        [master_secret_key.hi, master_secret_key.lo, app_address.to_field()],\n        app_secret_generator,\n    )\n}\n\npub fn merkle_hash(left: Field, right: Field) -> Field {\n    poseidon2_hash([left, right])\n}\n\npub fn compute_l2_to_l1_hash(\n    contract_address: AztecAddress,\n    recipient: EthAddress,\n    content: Field,\n    rollup_version_id: Field,\n    chain_id: Field,\n) -> Field {\n    let contract_address_bytes: [u8; 32] = contract_address.to_field().to_be_bytes();\n    let recipient_bytes: [u8; 20] = recipient.to_be_bytes();\n    let content_bytes: [u8; 32] = content.to_be_bytes();\n    let rollup_version_id_bytes: [u8; 32] = rollup_version_id.to_be_bytes();\n    let chain_id_bytes: [u8; 32] = chain_id.to_be_bytes();\n\n    let mut bytes: [u8; 148] = std::mem::zeroed();\n    for i in 0..32 {\n        bytes[i] = contract_address_bytes[i];\n        bytes[i + 32] = rollup_version_id_bytes[i];\n        // 64 - 84 are for recipient.\n        bytes[i + 84] = chain_id_bytes[i];\n        bytes[i + 116] = content_bytes[i];\n    }\n\n    for i in 0..20 {\n        bytes[64 + i] = recipient_bytes[i];\n    }\n\n    sha256_to_field(bytes)\n}\n\npub fn silo_l2_to_l1_message(\n    msg: Scoped<L2ToL1Message>,\n    rollup_version_id: Field,\n    chain_id: Field,\n) -> Field {\n    if msg.contract_address.is_zero() {\n        0\n    } else {\n        compute_l2_to_l1_hash(\n            msg.contract_address,\n            msg.inner.recipient,\n            msg.inner.content,\n            rollup_version_id,\n            chain_id,\n        )\n    }\n}\n\n// Computes sha256 hash of 2 input hashes.\n//\n// NB: This method now takes in two 31 byte fields - it assumes that any input\n// is the result of a sha_to_field hash and => is truncated\n//\n// TODO(Jan and David): This is used for the encrypted_log hashes.\n// Can we check to see if we can just use hash_to_field or pedersen_compress here?\n//\npub fn accumulate_sha256(input: [Field; 2]) -> Field {\n    // This is a note about the cpp code, since it takes an array of Fields\n    // instead of a u128.\n    // 4 Field elements when converted to bytes will usually\n    // occupy 4 * 32 = 128 bytes.\n    // However, this function is making the assumption that each Field\n    // only occupies 128 bits.\n    //\n    // TODO(David): This does not seem to be getting guaranteed anywhere in the code?\n    // Concatentate two fields into 32x2 = 64 bytes\n    // accumulate_sha256 assumes that the inputs are pre-truncated 31 byte numbers\n    let mut hash_input_flattened = [0; 64];\n    for offset in 0..input.len() {\n        let input_as_bytes: [u8; 32] = input[offset].to_be_bytes();\n        for byte_index in 0..32 {\n            hash_input_flattened[offset * 32 + byte_index] = input_as_bytes[byte_index];\n        }\n    }\n\n    sha256_to_field(hash_input_flattened)\n}\n\n#[inline_always]\npub fn pedersen_hash<let N: u32>(inputs: [Field; N], hash_index: u32) -> Field {\n    std::hash::pedersen_hash_with_separator(inputs, hash_index)\n}\n\npub fn poseidon2_hash<let N: u32>(inputs: [Field; N]) -> Field {\n    poseidon::poseidon2::Poseidon2::hash(inputs, N)\n}\n\n#[no_predicates]\npub fn poseidon2_hash_with_separator<let N: u32, T>(inputs: [Field; N], separator: T) -> Field\nwhere\n    T: ToField,\n{\n    let inputs_with_separator = array_concat([separator.to_field()], inputs);\n    poseidon2_hash(inputs_with_separator)\n}\n\n// Performs a fixed length hash with a subarray of the given input.\n// Useful for SpongeBlob in which we aborb M things and want to check it vs a hash of M elts of an N-len array.\n// Using stdlib poseidon, this will always absorb an extra 1 as a 'variable' hash, and not match spongeblob.squeeze()\n// or any ts implementation. Also checks that any remaining elts not hashed are empty.\n#[no_predicates]\npub fn poseidon2_hash_subarray<let N: u32>(input: [Field; N], in_len: u32) -> Field {\n    let mut sponge = poseidon2_absorb_chunks(input, in_len, false);\n    sponge.squeeze()\n}\n\n// NB the below is the same as poseidon::poseidon2::Poseidon2::hash(), but replacing a range check with a bit check,\n// and absorbing in chunks of 3 below.\n#[no_predicates]\npub fn poseidon2_cheaper_variable_hash<let N: u32>(input: [Field; N], in_len: u32) -> Field {\n    let mut sponge = poseidon2_absorb_chunks(input, in_len, true);\n    // In the case where the hash preimage is variable-length, we append `1` to the end of the input, to distinguish\n    // from fixed-length hashes. (the combination of this additional field element + the hash IV ensures\n    // fixed-length and variable-length hashes do not collide)\n    if in_len != N {\n        sponge.absorb(1);\n    }\n    sponge.squeeze()\n}\n\n// The below fn reduces gates of a conditional poseidon2 hash by approx 3x (thank you ~* Giant Brain Dev @IlyasRidhuan *~ for the idea)\n// Why? Because when we call stdlib poseidon, we call absorb for each item. When absorbing is conditional, it seems the compiler does not know\n// what cache_size will be when calling absorb, so it assigns the permutation gates for /each i/ rather than /every 3rd i/, which is actually required.\n// The below code forces the compiler to:\n//  - absorb normally up to 2 times to set cache_size to 1\n//  - absorb in chunks of 3 to ensure perm. only happens every 3rd absorb\n//  - absorb normally up to 2 times to add any remaining values to the hash\n// In fixed len hashes, the compiler is able to tell that it will only need to perform the permutation every 3 absorbs.\n// NB: it also replaces unnecessary range checks (i < thing) with a bit check (&= i != thing), which alone reduces the gates of a var. hash by half.\n\n#[no_predicates]\nfn poseidon2_absorb_chunks<let N: u32>(\n    input: [Field; N],\n    in_len: u32,\n    variable: bool,\n) -> Poseidon2Sponge {\n    let iv: Field = (in_len as Field) * TWO_POW_64;\n    let mut sponge = Poseidon2Sponge::new(iv);\n    // Even though shift is always 1 here, if we input in_len = 0 we get an underflow\n    // since we cannot isolate computation branches. The below is just to avoid that.\n    let shift = if in_len == 0 { 0 } else { 1 };\n    if in_len != 0 {\n        // cache_size = 0, init absorb\n        sponge.cache[0] = input[0];\n        sponge.cache_size = 1;\n        // shift = num elts already added to make cache_size 1 = 1 for a fresh sponge\n        // M = max_chunks = (N - 1 - (N - 1) % 3) / 3: (must be written as a fn of N to compile)\n        // max_remainder = (N - 1) % 3;\n        // max_chunks = (N - 1 - max_remainder) / 3;\n        sponge = poseidon2_absorb_chunks_loop::<N, (N - 1 - (N - 1) % 3) / 3>(\n            sponge,\n            input,\n            in_len,\n            variable,\n            shift,\n        );\n    }\n    sponge\n}\n\n// NB: If it's not required to check that the non-absorbed elts of 'input' are 0s, set skip_0_check=true\n#[no_predicates]\npub fn poseidon2_absorb_chunks_existing_sponge<let N: u32>(\n    in_sponge: Poseidon2Sponge,\n    input: [Field; N],\n    in_len: u32,\n    skip_0_check: bool,\n) -> Poseidon2Sponge {\n    let mut sponge = in_sponge;\n    // 'shift' is to account for already added inputs\n    let mut shift = 0;\n    // 'stop' is to avoid an underflow when inputting in_len = 0\n    let mut stop = false;\n    for i in 0..3 {\n        if shift == in_len {\n            stop = true;\n        }\n        if (sponge.cache_size != 1) & (!stop) {\n            sponge.absorb(input[i]);\n            shift += 1;\n        }\n    }\n    sponge = if stop {\n        sponge\n    } else {\n        // max_chunks = (N - (N % 3)) / 3;\n        poseidon2_absorb_chunks_loop::<N, (N - (N % 3)) / 3>(\n            sponge,\n            input,\n            in_len,\n            skip_0_check,\n            shift,\n        )\n    };\n    sponge\n}\n\n// The below is the loop to absorb elts into a poseidon sponge in chunks of 3\n// shift - the num of elts already absorbed to ensure the sponge's cache_size = 1\n// M - the max number of chunks required to absorb N things (must be comptime to compile)\n// NB: The 0 checks ('Found non-zero field...') are messy, but having a separate loop over N to check\n// for 0s costs 3N gates. Current approach is approx 2N gates.\n#[no_predicates]\nfn poseidon2_absorb_chunks_loop<let N: u32, let M: u32>(\n    in_sponge: Poseidon2Sponge,\n    input: [Field; N],\n    in_len: u32,\n    variable: bool,\n    shift: u32,\n) -> Poseidon2Sponge {\n    assert(in_len <= N, \"Given in_len to absorb is larger than the input array len\");\n    // When we have an existing sponge, we may have a shift of 0, and the final 'k+2' below = N\n    // The below avoids an overflow\n    let skip_last = 3 * M == N;\n    // Writing in_sponge: &mut does not compile\n    let mut sponge = in_sponge;\n    let mut should_add = true;\n    // The num of things left over after absorbing in 3s\n    let remainder = (in_len - shift) % 3;\n    // The num of chunks of 3 to absorb (maximum M)\n    let chunks = (in_len - shift - remainder) / 3;\n    for i in 0..M {\n        // Now we loop through cache size = 1 -> 3\n        should_add &= i != chunks;\n        // This is the index at the start of the chunk (for readability)\n        let k = 3 * i + shift;\n        if should_add {\n            // cache_size = 1, 2 => just assign\n            sponge.cache[1] = input[k];\n            sponge.cache[2] = input[k + 1];\n            // cache_size = 3 => duplex + perm\n            for j in 0..3 {\n                sponge.state[j] += sponge.cache[j];\n            }\n            sponge.state = std::hash::poseidon2_permutation(sponge.state, 4);\n            sponge.cache[0] = input[k + 2];\n            // cache_size is now 1 again, repeat loop\n        } else if (!variable) & (i != chunks) {\n            // if we are hashing a fixed len array which is a subarray, we check the remaining elts are 0\n            // NB: we don't check at i == chunks, because that chunk contains elts to be absorbed or checked below\n            let last_0 = if (i == M - 1) & (skip_last) {\n                0\n            } else {\n                input[k + 2]\n            };\n            let all_0 = (input[k] == 0) & (input[k + 1] == 0) & (last_0 == 0);\n            assert(all_0, \"Found non-zero field after breakpoint\");\n        }\n    }\n    // we have 'remainder' num of items left to absorb\n    should_add = true;\n    // below is to avoid overflows (i.e. if inlen is close to N)\n    let mut should_check = !variable;\n    for i in 0..3 {\n        should_add &= i != remainder;\n        should_check &= in_len - remainder + i != N;\n        if should_add {\n            // we want to absorb the final 'remainder' items\n            sponge.absorb(input[in_len - remainder + i]);\n        } else if should_check {\n            assert(input[in_len - remainder + i] == 0, \"Found non-zero field after breakpoint\");\n        }\n    }\n    sponge\n}\n\npub fn poseidon2_hash_with_separator_slice<T>(inputs: [Field], separator: T) -> Field\nwhere\n    T: ToField,\n{\n    let in_len = inputs.len() + 1;\n    let iv: Field = (in_len as Field) * TWO_POW_64;\n    let mut sponge = Poseidon2Sponge::new(iv);\n    sponge.absorb(separator.to_field());\n\n    for i in 0..inputs.len() {\n        sponge.absorb(inputs[i]);\n    }\n\n    sponge.squeeze()\n}\n\n// This function is  unconstrained because it is intended to be used in unconstrained context only as\n// in constrained contexts it would be too inefficient.\npub unconstrained fn poseidon2_hash_with_separator_bounded_vec<let N: u32, T>(\n    inputs: BoundedVec<Field, N>,\n    separator: T,\n) -> Field\nwhere\n    T: ToField,\n{\n    let in_len = inputs.len() + 1;\n    let iv: Field = (in_len as Field) * TWO_POW_64;\n    let mut sponge = Poseidon2Sponge::new(iv);\n    sponge.absorb(separator.to_field());\n\n    for i in 0..inputs.len() {\n        sponge.absorb(inputs.get(i));\n    }\n\n    sponge.squeeze()\n}\n\n#[no_predicates]\npub fn poseidon2_hash_bytes<let N: u32>(inputs: [u8; N]) -> Field {\n    let mut fields = [0; (N + 30) / 31];\n    let mut field_index = 0;\n    let mut current_field = [0; 31];\n    for i in 0..inputs.len() {\n        let index = i % 31;\n        current_field[index] = inputs[i];\n        if index == 30 {\n            fields[field_index] = field_from_bytes(current_field, false);\n            current_field = [0; 31];\n            field_index += 1;\n        }\n    }\n    if field_index != fields.len() {\n        fields[field_index] = field_from_bytes(current_field, false);\n    }\n    poseidon2_hash(fields)\n}\n\n#[test]\nfn poseidon_chunks_matches_fixed() {\n    let in_len = 501;\n    let mut input: [Field; 4096] = [0; 4096];\n    let mut fixed_input = [3; 501];\n    assert(in_len == fixed_input.len()); // sanity check\n    for i in 0..in_len {\n        input[i] = 3;\n    }\n    let sub_chunk_hash = poseidon2_hash_subarray(input, in_len);\n    let fixed_len_hash = poseidon::poseidon2::Poseidon2::hash(fixed_input, fixed_input.len());\n    assert(sub_chunk_hash == fixed_len_hash);\n}\n\n#[test]\nfn poseidon_chunks_matches_variable() {\n    let in_len = 501;\n    let mut input: [Field; 4096] = [0; 4096];\n    for i in 0..in_len {\n        input[i] = 3;\n    }\n    let variable_chunk_hash = poseidon2_cheaper_variable_hash(input, in_len);\n    let variable_len_hash = poseidon::poseidon2::Poseidon2::hash(input, in_len);\n    assert(variable_chunk_hash == variable_len_hash);\n}\n\n#[test]\nfn existing_sponge_poseidon_chunks_matches_fixed() {\n    let in_len = 501;\n    let mut input: [Field; 4096] = [0; 4096];\n    let mut fixed_input = [3; 501];\n    assert(in_len == fixed_input.len()); // sanity check\n    for i in 0..in_len {\n        input[i] = 3;\n    }\n    // absorb 250 of the 501 things\n    let empty_sponge = Poseidon2Sponge::new((in_len as Field) * TWO_POW_64);\n    let first_sponge = poseidon2_absorb_chunks_existing_sponge(empty_sponge, input, 250, true);\n    // now absorb the final 251 (since they are all 3s, im being lazy and not making a new array)\n    let mut final_sponge = poseidon2_absorb_chunks_existing_sponge(first_sponge, input, 251, true);\n    let fixed_len_hash = Poseidon2Sponge::hash(fixed_input, fixed_input.len());\n    assert(final_sponge.squeeze() == fixed_len_hash);\n}\n\n#[test]\nfn poseidon_chunks_empty_inputs() {\n    let in_len = 0;\n    let mut input: [Field; 4096] = [0; 4096];\n    let mut constructed_empty_sponge = poseidon2_absorb_chunks(input, in_len, true);\n    let mut first_sponge =\n        poseidon2_absorb_chunks_existing_sponge(constructed_empty_sponge, input, in_len, true);\n    assert(first_sponge.squeeze() == constructed_empty_sponge.squeeze());\n}\n\n#[test]\nfn smoke_sha256_to_field() {\n    let full_buffer = [\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n        25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n        48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n        71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93,\n        94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n        131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148,\n        149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,\n    ];\n    let result = sha256_to_field(full_buffer);\n\n    assert(result == 0x448ebbc9e1a31220a2f3830c18eef61b9bd070e5084b7fa2a359fe729184c7);\n\n    // to show correctness of the current ver (truncate one byte) vs old ver (mod full bytes):\n    let result_bytes = sha256::digest(full_buffer);\n    let truncated_field = crate::utils::field::field_from_bytes_32_trunc(result_bytes);\n    assert(truncated_field == result);\n    let mod_res = result + (result_bytes[31] as Field);\n    assert(mod_res == 0x448ebbc9e1a31220a2f3830c18eef61b9bd070e5084b7fa2a359fe729184e0);\n}\n\n#[test]\nfn compute_l2_l1_hash() {\n    // All zeroes\n    let hash_result =\n        compute_l2_to_l1_hash(AztecAddress::from_field(0), EthAddress::zero(), 0, 0, 0);\n    assert(hash_result == 0x3b18c58c739716e76429634a61375c45b3b5cd470c22ab6d3e14cee23dd992);\n\n    // Non-zero case\n    let hash_result = compute_l2_to_l1_hash(\n        AztecAddress::from_field(1),\n        EthAddress::from_field(3),\n        5,\n        2,\n        4,\n    );\n    assert(hash_result == 0xaab2a5828156782b12a1dc6f336e2bc627eb1b9514b02d511f66296990c050);\n}\n\n#[test]\nfn silo_l2_to_l1_message_matches_typescript() {\n    let version = 4;\n    let chainId = 5;\n\n    let hash = silo_l2_to_l1_message(\n        L2ToL1Message { recipient: EthAddress::from_field(1), content: 2 }.scope(\n            AztecAddress::from_field(3),\n        ),\n        version,\n        chainId,\n    );\n\n    // The following value was generated by `yarn-project/stdlib/src/hash/hash.test.ts`\n    let hash_from_typescript = 0x0081edf209e087ad31b3fd24263698723d57190bd1d6e9fe056fc0c0a68ee661;\n\n    assert_eq(hash, hash_from_typescript);\n}\n\n#[test]\nunconstrained fn poseidon2_hash_with_separator_bounded_vec_matches_non_bounded_vec_version() {\n    let inputs = BoundedVec::<Field, 4>::from_array([1, 2, 3]);\n    let separator = 42;\n\n    // Hash using bounded vec version\n    let bounded_result = poseidon2_hash_with_separator_bounded_vec(inputs, separator);\n\n    // Hash using regular version\n    let regular_result = poseidon2_hash_with_separator([1, 2, 3], separator);\n\n    // Results should match\n    assert_eq(bounded_result, regular_result);\n}\n","path":"/Users/satyam/nargo/github.com/AztecProtocol/aztec-packages/v1.1.2/noir-projects/noir-protocol-circuits/crates/types/src/hash.nr"},"312":{"source":"use crate::{hash::merkle_hash, merkle_tree::merkle_tree::MerkleTree};\n\n// Calculate the Merkle tree root from the sibling path and leaf.\n//\n// The leaf is hashed with its sibling, and then the result is hashed\n// with the next sibling etc in the path. The last hash is the root.\n//\n// TODO(David/Someone): The cpp code is using a uint256, whereas its\n// TODO a bit simpler in Noir to just have a bit array.\n// TODO: I'd generally like to avoid u256 for algorithms like\n// this because it means we never even need to consider cases where\n// the index is greater than p.\npub fn root_from_sibling_path<let N: u32>(\n    leaf: Field,\n    leaf_index: Field,\n    sibling_path: [Field; N],\n) -> Field {\n    let mut node = leaf;\n    let indices: [u1; N] = leaf_index.to_le_bits();\n\n    for i in 0..N {\n        let (hash_left, hash_right) = if indices[i] == 1 {\n            (sibling_path[i], node)\n        } else {\n            (node, sibling_path[i])\n        };\n        node = merkle_hash(hash_left, hash_right);\n    }\n    node\n}\n\npub fn calculate_subtree_root<let N: u32>(leaves: [Field; N]) -> Field {\n    MerkleTree::new(leaves).get_root()\n}\n\n// These values are precomputed and we run tests to ensure that they\n// are correct. The values themselves were computed from the cpp code.\n//\n// Would be good if we could use width since the compute_subtree\n// algorithm uses depth.\npub fn calculate_empty_tree_root(depth: u32) -> Field {\n    if depth == 0 {\n        0\n    } else if depth == 1 {\n        0x0b63a53787021a4a962a452c2921b3663aff1ffd8d5510540f8e659e782956f1\n    } else if depth == 2 {\n        0x0e34ac2c09f45a503d2908bcb12f1cbae5fa4065759c88d501c097506a8b2290\n    } else if depth == 3 {\n        0x21f9172d72fdcdafc312eee05cf5092980dda821da5b760a9fb8dbdf607c8a20\n    } else if depth == 4 {\n        0x2373ea368857ec7af97e7b470d705848e2bf93ed7bef142a490f2119bcf82d8e\n    } else if depth == 5 {\n        0x120157cfaaa49ce3da30f8b47879114977c24b266d58b0ac18b325d878aafddf\n    } else if depth == 6 {\n        0x01c28fe1059ae0237b72334700697bdf465e03df03986fe05200cadeda66bd76\n    } else if depth == 7 {\n        0x2d78ed82f93b61ba718b17c2dfe5b52375b4d37cbbed6f1fc98b47614b0cf21b\n    } else if depth == 8 {\n        0x067243231eddf4222f3911defbba7705aff06ed45960b27f6f91319196ef97e1\n    } else if depth == 9 {\n        0x1849b85f3c693693e732dfc4577217acc18295193bede09ce8b97ad910310972\n    } else if depth == 10 {\n        0x2a775ea761d20435b31fa2c33ff07663e24542ffb9e7b293dfce3042eb104686\n    } else {\n        panic(f\"depth should be between 0 and 10\")\n    }\n}\n\n#[test]\nfn test_merkle_root_interop_test() {\n    // This is a test to ensure that we match the cpp implementation.\n    // You can grep for `TEST_F(root_rollup_tests, noir_interop_test)`\n    // to find the test that matches this.\n    let root = calculate_subtree_root([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]);\n    assert(0x1a09d935ae110b4c861fcec8f9099ec30b4485022aeb3d3cf9d7168e38fdc231 == root);\n\n    let empty_root = calculate_subtree_root([0; 16]);\n    assert(0x2373ea368857ec7af97e7b470d705848e2bf93ed7bef142a490f2119bcf82d8e == empty_root);\n}\n\n#[test]\nfn test_empty_subroot() {\n    assert(calculate_empty_tree_root(0) == 0);\n\n    let expected_empty_root_2 = calculate_subtree_root([0; 2]);\n    assert(calculate_empty_tree_root(1) == expected_empty_root_2);\n\n    let expected_empty_root_4 = calculate_subtree_root([0; 4]);\n    assert(calculate_empty_tree_root(2) == expected_empty_root_4);\n\n    let expected_empty_root_8 = calculate_subtree_root([0; 8]);\n    assert(calculate_empty_tree_root(3) == expected_empty_root_8);\n\n    let expected_empty_root_16 = calculate_subtree_root([0; 16]);\n    assert(calculate_empty_tree_root(4) == expected_empty_root_16);\n\n    let expected_empty_root_32 = calculate_subtree_root([0; 32]);\n    assert(calculate_empty_tree_root(5) == expected_empty_root_32);\n\n    let expected_empty_root_64 = calculate_subtree_root([0; 64]);\n    assert(calculate_empty_tree_root(6) == expected_empty_root_64);\n\n    let expected_empty_root_128 = calculate_subtree_root([0; 128]);\n    assert(calculate_empty_tree_root(7) == expected_empty_root_128);\n\n    let expected_empty_root_256 = calculate_subtree_root([0; 256]);\n    assert(calculate_empty_tree_root(8) == expected_empty_root_256);\n\n    let expected_empty_root_512 = calculate_subtree_root([0; 512]);\n    assert(calculate_empty_tree_root(9) == expected_empty_root_512);\n\n    let expected_empty_root_1024 = calculate_subtree_root([0; 1024]);\n    assert(calculate_empty_tree_root(10) == expected_empty_root_1024);\n}\n","path":"/Users/satyam/nargo/github.com/AztecProtocol/aztec-packages/v1.1.2/noir-projects/noir-protocol-circuits/crates/types/src/merkle_tree/root.nr"},"340":{"source":"use crate::{hash::poseidon2_hash, traits::ToField};\n\npub fn derive_storage_slot_in_map<K>(storage_slot: Field, key: K) -> Field\nwhere\n    K: ToField,\n{\n    poseidon2_hash([storage_slot, key.to_field()])\n}\n\nmod test {\n    use crate::{address::AztecAddress, storage::map::derive_storage_slot_in_map, traits::FromField};\n\n    #[test]\n    fn test_derive_storage_slot_in_map_matches_typescript() {\n        let map_slot = 0x132258fb6962c4387ba659d9556521102d227549a386d39f0b22d1890d59c2b5;\n        let key = AztecAddress::from_field(\n            0x302dbc2f9b50a73283d5fb2f35bc01eae8935615817a0b4219a057b2ba8a5a3f,\n        );\n\n        let slot = derive_storage_slot_in_map(map_slot, key);\n\n        // The following value was generated by `map_slot.test.ts`\n        let slot_from_typescript =\n            0x15b9fe39449affd8b377461263e9d2b610b9ad40580553500b4e41d9cbd887ac;\n\n        assert_eq(slot, slot_from_typescript);\n    }\n}\n","path":"/Users/satyam/nargo/github.com/AztecProtocol/aztec-packages/v1.1.2/noir-projects/noir-protocol-circuits/crates/types/src/storage/map.nr"},"392":{"source":"use aztec::{\n    note::{\n        note_getter::get_notes,\n        note_interface::{NoteHash, NoteType},\n        note_metadata::{NoteMetadata, SettledNoteMetadata},\n        retrieved_note::RetrievedNote,\n        utils::compute_note_hash_for_nullify,\n    },\n    prelude::{AztecAddress, NoteGetterOptions, PrivateContext},\n    protocol_types::{\n        constants::NOTE_HASH_TREE_HEIGHT,\n        merkle_tree::{membership::MembershipWitness, root::root_from_sibling_path},\n        traits::{Packable, Serialize},\n    },\n    state_vars::storage::Storage,\n};\n\npub fn assert_note_inclusion<NOTE>(\n    note: NOTE,\n    note_nonce: Field,\n    membership_witness: MembershipWitness<NOTE_HASH_TREE_HEIGHT>,\n    contract_address: AztecAddress,\n    storage_slot: Field,\n    note_hash_tree_root: Field,\n)\nwhere\n    NOTE: NoteHash,\n{\n    // storage proof\n    let retrieved_note = RetrievedNote {\n        note,\n        contract_address,\n        metadata: NoteMetadata::from(SettledNoteMetadata::new(note_nonce)),\n    };\n    let note_hash = compute_note_hash_for_nullify(retrieved_note, storage_slot);\n    let computed_root = root_from_sibling_path(\n        note_hash,\n        membership_witness.leaf_index,\n        membership_witness.sibling_path,\n    );\n    assert(\n        computed_root == note_hash_tree_root,\n        f\"root mismatch: {computed_root} != {note_hash_tree_root}\",\n    );\n}\n\npub fn get_note_inclusion_data<NOTE, let N: u32, PREPROCESSOR_ARGS, FILTER_ARGS>(\n    context: &mut PrivateContext,\n    storage_var: impl Storage<_>,\n    options: NoteGetterOptions<NOTE, _, PREPROCESSOR_ARGS, FILTER_ARGS>,\n) -> NoteInclusionData<NOTE>\nwhere\n    NOTE: NoteType + NoteHash + Packable<N> + Eq,\n{\n    let storage_slot = storage_var.get_storage_slot();\n    let (retrieved_notes, _): (BoundedVec<RetrievedNote<NOTE>, _>, _) =\n        get_notes::<NOTE, _, PREPROCESSOR_ARGS, FILTER_ARGS>(context, storage_slot, options);\n    let note = retrieved_notes.get(0);\n    let note_hash = compute_note_hash_for_nullify(note, storage_slot);\n    NoteInclusionData { note, note_hash }\n}\n\npub struct NoteInclusionData<Note> {\n    pub note: RetrievedNote<Note>,\n    pub note_hash: Field,\n}\n\nimpl<Note, let N: u32> Serialize<N + 3 + 1> for NoteInclusionData<Note>\nwhere\n    Note: Serialize<N>,\n{\n    fn serialize(self) -> [Field; N + 3 + 1] {\n        let a = self.note.serialize();\n        let b = a.concat([self.note_hash]);\n        b\n    }\n}\n","path":"/Users/satyam/nargo/github.com/nemi-fi/aztec_storage_proofs/v0.7.0/lib/src/lib.nr"}},"names":["main"],"brillig_names":["directive_invert","directive_to_radix"]}